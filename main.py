import os
import json
import torch
import random
import numpy as np
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup


from config import CFG
from dataset import DNABERTDataset
from tokenizer import KmerTokenizer  # ä½ è‡ªå·±å®šä¹‰çš„åˆ†è¯å™¨ç±»
from model import get_dnabert_model  # ä½ è‡ªå·±å®šä¹‰çš„æ¨¡å‹è·å–å‡½æ•°


# ===== 1. è®¾ç½®éšæœºç§å­ =====
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

from tqdm import tqdm  

# ===== 2. è®­ç»ƒå‡½æ•° =====
def train_fn(model, dataloader, optimizer, scheduler):
    model.train()
    total_loss = 0

    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Training', leave=True)

    for step, batch in progress_bar:
        input_ids = batch['input_ids'].to(CFG.device)
        attention_mask = batch['attention_mask'].to(CFG.device)
        labels = batch['labels'].to(CFG.device)

        outputs = model(input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels)

        loss = outputs.loss
        loss = loss.mean()
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        progress_bar.set_postfix({'step': step + 1, 'loss': f"{loss.item():.4f}"})

    return total_loss / len(dataloader)

# ===== 3. ä¸»å‡½æ•° =====
def main():
    # set seed
    set_seed(CFG.seed)

    # load vocab
    with open(CFG.vocab_path, 'r') as f:
        vocab_dict = json.load(f)

    # initialize tokenizer
    tokenizer = KmerTokenizer(vocab_dict=vocab_dict, k=CFG.k)

    # Dataset & Dataloader
    train_dataset = DNABERTDataset(CFG.train_path, tokenizer, CFG.max_len)
    train_loader = DataLoader(train_dataset,
                              batch_size=CFG.batch_size,
                              shuffle=True,
                              num_workers=CFG.num_workers)

    # æ¨¡å‹
    model = get_dnabert_model(vocab_dict, CFG)
    model.to(CFG.device)

    # å¤šå¡å¹¶è¡Œ
    is_parallel = False
    if torch.cuda.device_count() > 1:
        print(f"ğŸ”§ ä½¿ç”¨ {torch.cuda.device_count()} å—GPUå¹¶è¡Œè®­ç»ƒ")
        model = torch.nn.DataParallel(model)
        is_parallel = True

    # Optimizer & Scheduler
    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)
    total_steps = len(train_loader) * CFG.epochs
    warmup_steps = int(CFG.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=warmup_steps,
                                                num_training_steps=total_steps)

    # è®­ç»ƒ
    for epoch in range(CFG.epochs):
        print(f"\n--- Epoch {epoch + 1}/{CFG.epochs} ---")
        avg_loss = train_fn(model, train_loader, optimizer, scheduler)
        print(f"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}")

    # ===========================
    # âœ… æ¨¡å‹ä¿å­˜ï¼ˆç»“æ„ + æƒé‡ + tokenizer + optimizerï¼‰
    # ===========================
    os.makedirs(CFG.output_dir, exist_ok=True)

    # ä¿å­˜æ¨¡å‹ç»“æ„å’Œæƒé‡
    if is_parallel:
        model.module.save_pretrained(CFG.output_dir)
    else:
        model.save_pretrained(CFG.output_dir)

    # ä¿å­˜ tokenizerï¼ˆä½ è‡ªå®šä¹‰çš„ KmerTokenizerï¼Œè¿™é‡Œåªä¿å­˜ vocabï¼‰
    with open(os.path.join(CFG.output_dir, f"vocab{CFG.k}.json"), "w") as f:
        json.dump(tokenizer.vocab, f, indent=4)
    with open(os.path.join(CFG.output_dir, "tokenizer_config.json"), "w") as f:
        json.dump({"k": CFG.k}, f, indent=4)

    # ä¿å­˜ optimizer å’Œ schedulerï¼ˆå¯é€‰ï¼‰
    torch.save({
        "optimizer_state_dict": optimizer.state_dict(),
        "scheduler_state_dict": scheduler.state_dict(),
    }, os.path.join(CFG.output_dir, "optimizer_scheduler.pt"))

    # ä¿å­˜è®­ç»ƒå‚æ•°
    train_args = {
        # åŸºæœ¬è®¾ç½®
        "project": CFG.project,
        "seed": CFG.seed,

        # æ•°æ®è®¾ç½®
        "k": CFG.k,
        "max_len": CFG.max_len,
        "train_path": CFG.train_path,
        "vocab_path": CFG.vocab_path,

        # æ¨¡å‹è®¾ç½®
        "hidden_size": CFG.hidden_size,
        "num_hidden_layers": CFG.num_hidden_layers,
        "num_attention_heads": CFG.num_attention_heads,
        "intermediate_size": CFG.intermediate_size,
        "type_vocab_size": CFG.type_vocab_size,

        # è®­ç»ƒè®¾ç½®
        "batch_size": CFG.batch_size,
        "num_workers": CFG.num_workers,
        "epochs": CFG.epochs,
        "lr": CFG.lr,
        "weight_decay": CFG.weight_decay,
        "warmup_ratio": CFG.warmup_ratio,
        "max_grad_norm": CFG.max_grad_norm,

        # ä¿å­˜è®¾ç½®
        "output_dir": CFG.output_dir,
        "save_steps": CFG.save_steps,
        "logging_steps": CFG.logging_steps,

        # å…¶ä»–
        "use_fp16": CFG.use_fp16
    }

    with open(os.path.join(CFG.output_dir, "train_args.json"), "w") as f:
        json.dump(train_args, f, indent=4)

    print(f"âœ… æ¨¡å‹ä¸ç›¸å…³ä¿¡æ¯å·²ä¿å­˜åˆ°ï¼š{CFG.output_dir}")



if __name__ == '__main__':
    main()
    print("ğŸš€ è®­ç»ƒå®Œæˆï¼")