{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12988d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecefd4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:5-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99147424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from config import CFG\n",
    "from dataset import DNABERTDataset\n",
    "from tokenizer import KmerTokenizer  # 你自己定义的分词器类\n",
    "from model import get_dnabert_model  # 你自己定义的模型获取函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e826468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 39101\n",
      "train dataloader size: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\nur\\developers\\Anaconda_envs\\envs\\DNABERT\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:624: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "with open(CFG.vocab_path, 'r') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = KmerTokenizer(vocab_dict=vocab_dict, k=CFG.k)\n",
    "\n",
    "# Dataset & Dataloader\n",
    "train_dataset = DNABERTDataset(CFG.train_path, tokenizer, CFG.max_len)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=CFG.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=CFG.num_workers)\n",
    "\n",
    "print(f\"train dataset size: {len(train_dataset)}\")\n",
    "print(f\"train dataloader size: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0181f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps: 3900, warmup steps: 390\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * CFG.epochs\n",
    "warmup_steps = int(CFG.warmup_ratio * total_steps)  # 比如 0.1 表示 10%\n",
    "print(f\"total steps: {total_steps}, warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee041de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNABERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
